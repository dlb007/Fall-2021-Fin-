{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# For this exercise, you should start by reading in the text file produced from Exercise Three. This text file should already have the HTML code elements removed, \r\n",
    "# and primarily consist of text and other characters that we will remove through our processing. Store the input of your file in a string, \r\n",
    "# and convert the contents to lower case for consistency. (Note: setup from the week before focus)\r\n",
    "\r\n",
    "f = open('hollywoodreviews.txt','r')\r\n",
    "fileText = str(f.read().lower())\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import networkX as nx\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkX'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12556/3956178131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkX\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'networkX'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# This week, we'll be using functions that we've already defined in our exercises. Try looking back through your code and notes from class. \r\n",
    "# Our first step is to import the regular expressions module and remove all non-alpha-numeric characters from the string, then save the string as an array of words ready for processing. \r\n",
    "# (Use the code in NormalizingText for reference -accomplished!)\r\n",
    "\r\n",
    "def preprocess_text(text):\r\n",
    "    import re\r\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text)\r\n",
    "\r\n",
    "word_bag = preprocess_text(fileText)\r\n",
    "print(word_bag[0:50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['got', 'a', 'tip', 'definitive', 'voice', 'of', 'entertainment', 'hollywood', 'hollywood', 'hollywood', 'reportersee', 'my', 'optionssign', 'up', 'homemoviesmovie', 'newsafm', 'hidden', 'gem', 'how', 'pandemic', 'set', 'rom', 'com', 'was', 'directed', 'entirely', 'remotelyhow', 'pandemic', 'set', 'rom', 'com', 'mycorona', 'was', 'directed', 'remotely', 'by', 'helmer', 'phil', 'gorn', 'by', 'alex', 'ritmanplus', 'iconalex', 'ritmanu', 'k', 'correspondentmore', 'stories', 'by', 'alexrupert', 'murdoch']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Next, we'll use stop words to remove all the words that we don't want to include in our count. There are suggested words in this week's readings, \r\n",
    "# but you can also generate a custom list based on your topic. Define the stop words in an array, \r\n",
    "# and use a loop to remove any word in your bag of words that also appears on the stop words list (use the examples in Dictionaries for guidance.) (Note: So, many options, went back and continued to alter choices)\r\n",
    "\r\n",
    "stop_words = ['a','of','my','up','rom','com','how','was','remotelyhow','set','was','by','k','ritmanplus','iconalex','ritmanu','k','correspondentmore', 'got','tip','u','for','and']\r\n",
    "def removeStopWords(word_bag, stop_words):\r\n",
    "    return [w for w in word_bag if w not in stop_words]\r\n",
    "\r\n",
    "cleaned_words = removeStopWords(word_bag, stop_words)\r\n",
    "\r\n",
    "print(cleaned_words[0:50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['definitive', 'voice', 'entertainment', 'hollywood', 'hollywood', 'hollywood', 'reportersee', 'optionssign', 'homemoviesmovie', 'newsafm', 'hidden', 'gem', 'pandemic', 'directed', 'entirely', 'pandemic', 'mycorona', 'directed', 'remotely', 'helmer', 'phil', 'gorn', 'alex', 'stories', 'alexrupert', 'murdoch', 'piers', 'morgan', 'join', 'forces', 'new', 'network', 'warformer', 'sky', 'drama', 'head', 'cameron', 'roach', 'launches', 'production', 'company', 'rope', 'ladder', 'fictionbrian', 'cox', 'jodie', 'turner', 'smith', 'teaming', 'political']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Now we're ready to count our words, and move from an array to a dictionary. Using the functions we've already built in the \"Dictionaries.ipynb\" file, \r\n",
    "# process your text by building a dictionary that zips words with their frequency, then removes redundancy by storing the data in the \"dictionary\" format. (Note: Totals brought interest toward project influence)\r\n",
    "\r\n",
    "def wordsToDictionary(word_bag):\r\n",
    "    word_freq = [word_bag.count(word) for word in word_bag]\r\n",
    "    return dict(list(zip(word_bag,word_freq)))\r\n",
    "\r\n",
    "dict_words = wordsToDictionary(cleaned_words)\r\n",
    "\r\n",
    "print(dict_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'definitive': 1, 'voice': 1, 'entertainment': 2, 'hollywood': 3, 'reportersee': 1, 'optionssign': 1, 'homemoviesmovie': 1, 'newsafm': 1, 'hidden': 1, 'gem': 1, 'pandemic': 2, 'directed': 2, 'entirely': 1, 'mycorona': 1, 'remotely': 1, 'helmer': 1, 'phil': 1, 'gorn': 1, 'alex': 1, 'stories': 1, 'alexrupert': 1, 'murdoch': 1, 'piers': 1, 'morgan': 1, 'join': 1, 'forces': 1, 'new': 1, 'network': 1, 'warformer': 1, 'sky': 1, 'drama': 1, 'head': 1, 'cameron': 1, 'roach': 1, 'launches': 1, 'production': 1, 'company': 1, 'rope': 1, 'ladder': 1, 'fictionbrian': 1, 'cox': 1, 'jodie': 1, 'turner': 1, 'smith': 1, 'teaming': 1, 'political': 1, 'thriller': 1, '8216': 1, 'independent': 1, '8217': 1, 'exclusive': 1, 'view': 1, 'allnovember': 1, '12': 1, '2020': 1, '7': 1, '30amshare': 1, 'this': 10, 'article': 10, 'on': 10, 'facebookshare': 1, 'twittershare': 1, 'emailshow': 1, 'additional': 1, 'share': 1, 'optionsshare': 1, 'printshare': 1, 'commentshare': 1, 'whatsappshare': 1, 'linkedinshare': 1, 'redditshare': 1, 'pinitshare': 1, 'tumblra': 1, 'still': 1, 'from': 1, 'mycoronawonderphil': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Finally, explore what you can learn from this dictionary. (Using the sorting and unsorted dictionary: evoking different views of dat set) Try: \r\n",
    "# A. Sorting your dictionary using our prebuilt method (Key values making it applicable to look up words)\r\n",
    "\r\n",
    "def sortDictionary(words):\r\n",
    "    aux = [(words[key], key) for key in words]\r\n",
    "    aux.sort()\r\n",
    "    aux.reverse()\r\n",
    "    return aux\r\n",
    "\r\n",
    "sort_dict = sortDictionary(dict_words)\r\n",
    "\r\n",
    "# B. Printing the top ten most frequent words from your data\r\n",
    "x = 0\r\n",
    "for pair in sort_dict:\r\n",
    "    print(str(pair))\r\n",
    "    x += 1\r\n",
    "    if x==9:\r\n",
    "        break\r\n",
    "\r\n",
    "# C. Querying for certain key words and printing their frequency (Note: Great way to interact and setup)\r\n",
    "\r\n",
    "print(\"hollywood frequency: \" + str(dict_words.get(\"hollywood\")))\r\n",
    "print(\"pandemic frequency: \" + str(dict_words.get(\"pandemic\")))\r\n",
    "\r\n",
    "# D. Comparing the relative frequency of words of interest (Note: My favorite due to the comparison mode toward words used that were present)\r\n",
    "\r\n",
    "def compare_words(word_one, word_two):\r\n",
    "    if not word_one in dict_words or not word_two in dict_words:\r\n",
    "       print(\"Word(s) not found\")\r\n",
    "    elif dict_words[word_one] >  dict_words[word_two]:\r\n",
    "        print(word_one + \" appeared more often\")\r\n",
    "    elif dict_words[word_two] >  dict_words[word_one]:\r\n",
    "        print(word_two + \" appeared more often\") \r\n",
    "    else:\r\n",
    "        print(\"Words occurred with equal frequency\")\r\n",
    "\r\n",
    "compare_words(\"hollywood\",\"pandemic\")\r\n",
    "compare_words(\"pandemic\",\"fictionbrian\")\r\n",
    "compare_words(\"hollywood\",\"voice\")\r\n",
    "compare_words(\"forces\",\"sky\")\r\n",
    "compare_words(\"drama\",\"directed\")\r\n",
    "compare_words(\"independent\",\"thriller\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 'this')\n",
      "(10, 'on')\n",
      "(10, 'article')\n",
      "(3, 'hollywood')\n",
      "(2, 'pandemic')\n",
      "(2, 'entertainment')\n",
      "(2, 'directed')\n",
      "(1, 'whatsappshare')\n",
      "(1, 'warformer')\n",
      "hollywood frequency: 3\n",
      "pandemic frequency: 2\n",
      "hollywood appeared more often\n",
      "pandemic appeared more often\n",
      "hollywood appeared more often\n",
      "Words occurred with equal frequency\n",
      "directed appeared more often\n",
      "Words occurred with equal frequency\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f0bdea066720bef64c7d16dc91e76a199a58e1752ccc79d2521faa510c696e84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}